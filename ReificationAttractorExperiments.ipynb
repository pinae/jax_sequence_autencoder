{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32181c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: Matrix(3072, 3072) Vector(3072) \n",
      "Test Loss: 0.21524148 \tSSIM score: 0.0128818285\n",
      "Learning rate: 0.5\n",
      "training batch 0 loss: 0.28618854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 15:33:49.089314: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 1 loss: 0.22651051\n",
      "training batch 2 loss: 0.20398101\n",
      "training batch 3 loss: 0.19547589\n",
      "training batch 4 loss: 0.19226505\n",
      "training batch 5 loss: 0.1910529\n",
      "training batch 6 loss: 0.19059524\n",
      "training batch 7 loss: 0.19042253\n",
      "training batch 8 loss: 0.19035722\n",
      "training batch 9 loss: 0.19033273\n",
      "training batch 10 loss: 0.19032337\n",
      "training batch 11 loss: 0.19032\n",
      "training batch 12 loss: 0.19031857\n",
      "training batch 13 loss: 0.1903181\n",
      "training batch 14 loss: 0.1903178\n",
      "training batch 15 loss: 0.19031772\n",
      "training batch 16 loss: 0.19031772\n",
      "training batch 17 loss: 0.19031772\n",
      "training batch 18 loss: 0.19031772\n",
      "training batch 19 loss: 0.19031772\n",
      "training batch 20 loss: 0.19031772\n",
      "training batch 21 loss: 0.19031772\n",
      "training batch 22 loss: 0.19031772\n",
      "training batch 23 loss: 0.19031772\n",
      "training batch 24 loss: 0.19031772\n",
      "training batch 25 loss: 0.19031772\n",
      "training batch 26 loss: 0.19031772\n",
      "training batch 27 loss: 0.19031772\n",
      "training batch 28 loss: 0.19031772\n",
      "training batch 29 loss: 0.19031772\n",
      "training batch 30 loss: 0.19031772\n",
      "training batch 31 loss: 0.19031772\n",
      "training batch 32 loss: 0.19031772\n",
      "training batch 33 loss: 0.19031772\n",
      "training batch 34 loss: 0.19031772\n",
      "training batch 35 loss: 0.19031772\n",
      "training batch 36 loss: 0.19031772\n",
      "training batch 37 loss: 0.19031772\n",
      "training batch 38 loss: 0.19031772\n",
      "training batch 39 loss: 0.19031772\n",
      "training batch 40 loss: 0.19031772\n",
      "training batch 41 loss: 0.19031772\n",
      "training batch 42 loss: 0.19031772\n",
      "training batch 43 loss: 0.19031772\n",
      "training batch 44 loss: 0.19031772\n",
      "training batch 45 loss: 0.19031772\n",
      "training batch 46 loss: 0.19031772\n",
      "training batch 47 loss: 0.19031772\n",
      "training batch 48 loss: 0.19031772\n",
      "training batch 49 loss: 0.19031772\n",
      "training batch 50 loss: 0.19031772\n",
      "training batch 51 loss: 0.19031772\n",
      "training batch 52 loss: 0.19031772\n",
      "training batch 53 loss: 0.19031772\n",
      "training batch 54 loss: 0.19031772\n",
      "training batch 55 loss: 0.19031772\n",
      "training batch 56 loss: 0.19031772\n",
      "training batch 57 loss: 0.19031772\n",
      "training batch 58 loss: 0.19031772\n",
      "training batch 59 loss: 0.19031772\n",
      "training batch 60 loss: 0.19031772\n",
      "training batch 61 loss: 0.19031772\n",
      "training batch 62 loss: 0.19031772\n",
      "training batch 63 loss: 0.19031772\n",
      "training batch 64 loss: 0.19031772\n",
      "training batch 65 loss: 0.19031772\n",
      "training batch 66 loss: 0.19031772\n",
      "training batch 67 loss: 0.19031772\n",
      "training batch 68 loss: 0.19031772\n",
      "training batch 69 loss: 0.19031772\n",
      "training batch 70 loss: 0.19031772\n",
      "training batch 71 loss: 0.19031772\n",
      "training batch 72 loss: 0.19031772\n",
      "training batch 73 loss: 0.19031772\n",
      "training batch 74 loss: 0.19031772\n",
      "training batch 75 loss: 0.19031772\n",
      "training batch 76 loss: 0.19031772\n",
      "training batch 77 loss: 0.19031772\n",
      "training batch 78 loss: 0.19031772\n",
      "training batch 79 loss: 0.19031772\n",
      "training batch 80 loss: 0.19031772\n",
      "training batch 81 loss: 0.19031772\n",
      "training batch 82 loss: 0.19031772\n",
      "training batch 83 loss: 0.19031772\n",
      "training batch 84 loss: 0.19031772\n",
      "training batch 85 loss: 0.19031772\n",
      "training batch 86 loss: 0.19031772\n",
      "training batch 87 loss: 0.19031772\n",
      "training batch 88 loss: 0.19031772\n",
      "training batch 89 loss: 0.19031772\n",
      "training batch 90 loss: 0.19031772\n",
      "training batch 91 loss: 0.19031772\n",
      "training batch 92 loss: 0.19031772\n",
      "training batch 93 loss: 0.19031772\n",
      "training batch 94 loss: 0.19031772\n",
      "training batch 95 loss: 0.19031772\n",
      "training batch 96 loss: 0.19031772\n",
      "training batch 97 loss: 0.19031776\n",
      "Test Loss: 0.19081613 \tSSIM score: 0.0118470285\n",
      "Learning rate: 0.4\n",
      "training batch 0 loss: 0.19031772\n",
      "training batch 1 loss: 0.19031772\n",
      "training batch 2 loss: 0.19031772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 15:33:54.882751: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 3 loss: 0.19031772\n",
      "training batch 4 loss: 0.19031772\n",
      "training batch 5 loss: 0.19031772\n",
      "training batch 6 loss: 0.19031772\n",
      "training batch 7 loss: 0.19031772\n",
      "training batch 8 loss: 0.19031772\n",
      "training batch 9 loss: 0.19031772\n",
      "training batch 10 loss: 0.19031772\n",
      "training batch 11 loss: 0.19031772\n",
      "training batch 12 loss: 0.19031772\n",
      "training batch 13 loss: 0.19031772\n",
      "training batch 14 loss: 0.19031772\n",
      "training batch 15 loss: 0.19031772\n",
      "training batch 16 loss: 0.19031772\n",
      "training batch 17 loss: 0.19031772\n",
      "training batch 18 loss: 0.19031772\n",
      "training batch 19 loss: 0.19031772\n",
      "training batch 20 loss: 0.19031772\n",
      "training batch 21 loss: 0.19031772\n",
      "training batch 22 loss: 0.19031772\n",
      "training batch 23 loss: 0.19031772\n",
      "training batch 24 loss: 0.19031772\n",
      "training batch 25 loss: 0.19031772\n",
      "training batch 26 loss: 0.19031772\n",
      "training batch 27 loss: 0.19031772\n",
      "training batch 28 loss: 0.19031772\n",
      "training batch 29 loss: 0.19031772\n",
      "training batch 30 loss: 0.19031772\n",
      "training batch 31 loss: 0.19031772\n",
      "training batch 32 loss: 0.19031772\n",
      "training batch 33 loss: 0.19031772\n",
      "training batch 34 loss: 0.19031772\n",
      "training batch 35 loss: 0.19031772\n",
      "training batch 36 loss: 0.19031772\n",
      "training batch 37 loss: 0.19031772\n",
      "training batch 38 loss: 0.19031772\n",
      "training batch 39 loss: 0.19031772\n",
      "training batch 40 loss: 0.19031772\n",
      "training batch 41 loss: 0.19031772\n",
      "training batch 42 loss: 0.19031772\n",
      "training batch 43 loss: 0.19031772\n",
      "training batch 44 loss: 0.19031772\n",
      "training batch 45 loss: 0.19031772\n",
      "training batch 46 loss: 0.19031772\n",
      "training batch 47 loss: 0.19031772\n",
      "training batch 48 loss: 0.19031772\n",
      "training batch 49 loss: 0.19031772\n",
      "training batch 50 loss: 0.19031772\n",
      "training batch 51 loss: 0.19031772\n",
      "training batch 52 loss: 0.19031772\n",
      "training batch 53 loss: 0.19031772\n",
      "training batch 54 loss: 0.19031772\n",
      "training batch 55 loss: 0.19031772\n",
      "training batch 56 loss: 0.19031772\n",
      "training batch 57 loss: 0.19031772\n",
      "training batch 58 loss: 0.19031772\n",
      "training batch 59 loss: 0.19031772\n",
      "training batch 60 loss: 0.19031772\n",
      "training batch 61 loss: 0.19031772\n",
      "training batch 62 loss: 0.19031772\n",
      "training batch 63 loss: 0.19031772\n",
      "training batch 64 loss: 0.19031772\n",
      "training batch 65 loss: 0.19031772\n",
      "training batch 66 loss: 0.19031772\n",
      "training batch 67 loss: 0.19031772\n",
      "training batch 68 loss: 0.19031772\n",
      "training batch 69 loss: 0.19031772\n",
      "training batch 70 loss: 0.19031772\n",
      "training batch 71 loss: 0.19031772\n",
      "training batch 72 loss: 0.19031772\n",
      "training batch 73 loss: 0.19031772\n",
      "training batch 74 loss: 0.19031772\n",
      "training batch 75 loss: 0.19031772\n",
      "training batch 76 loss: 0.19031772\n",
      "training batch 77 loss: 0.19031772\n",
      "training batch 78 loss: 0.19031772\n",
      "training batch 79 loss: 0.19031772\n",
      "training batch 80 loss: 0.19031772\n",
      "training batch 81 loss: 0.19031772\n",
      "training batch 82 loss: 0.19031772\n",
      "training batch 83 loss: 0.19031772\n",
      "training batch 84 loss: 0.19031772\n",
      "training batch 85 loss: 0.19031772\n",
      "training batch 86 loss: 0.19031772\n",
      "training batch 87 loss: 0.19031772\n",
      "training batch 88 loss: 0.19031772\n",
      "training batch 89 loss: 0.19031772\n",
      "training batch 90 loss: 0.19031772\n",
      "training batch 91 loss: 0.19031772\n",
      "training batch 92 loss: 0.19031772\n",
      "training batch 93 loss: 0.19031772\n",
      "training batch 94 loss: 0.19031772\n",
      "training batch 95 loss: 0.19031772\n",
      "training batch 96 loss: 0.19031772\n",
      "training batch 97 loss: 0.19031776\n",
      "Test Loss: 0.19081613 \tSSIM score: 0.0118470285\n",
      "Learning rate: 0.32000000000000006\n",
      "training batch 0 loss: 0.19031772\n",
      "training batch 1 loss: 0.19031772\n",
      "training batch 2 loss: 0.19031772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 15:34:00.377136: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 3 loss: 0.19031772\n",
      "training batch 4 loss: 0.19031772\n",
      "training batch 5 loss: 0.19031772\n",
      "training batch 6 loss: 0.19031772\n",
      "training batch 7 loss: 0.19031772\n",
      "training batch 8 loss: 0.19031772\n",
      "training batch 9 loss: 0.19031772\n",
      "training batch 10 loss: 0.19031772\n",
      "training batch 11 loss: 0.19031772\n",
      "training batch 12 loss: 0.19031772\n",
      "training batch 13 loss: 0.19031772\n",
      "training batch 14 loss: 0.19031772\n",
      "training batch 15 loss: 0.19031772\n",
      "training batch 16 loss: 0.19031772\n",
      "training batch 17 loss: 0.19031772\n",
      "training batch 18 loss: 0.19031772\n",
      "training batch 19 loss: 0.19031772\n",
      "training batch 20 loss: 0.19031772\n",
      "training batch 21 loss: 0.19031772\n",
      "training batch 22 loss: 0.19031772\n",
      "training batch 23 loss: 0.19031772\n",
      "training batch 24 loss: 0.19031772\n",
      "training batch 25 loss: 0.19031772\n",
      "training batch 26 loss: 0.19031772\n",
      "training batch 27 loss: 0.19031772\n",
      "training batch 28 loss: 0.19031772\n",
      "training batch 29 loss: 0.19031772\n",
      "training batch 30 loss: 0.19031772\n",
      "training batch 31 loss: 0.19031772\n",
      "training batch 32 loss: 0.19031772\n",
      "training batch 33 loss: 0.19031772\n",
      "training batch 34 loss: 0.19031772\n",
      "training batch 35 loss: 0.19031772\n",
      "training batch 36 loss: 0.19031772\n",
      "training batch 37 loss: 0.19031772\n",
      "training batch 38 loss: 0.19031772\n",
      "training batch 39 loss: 0.19031772\n",
      "training batch 40 loss: 0.19031772\n",
      "training batch 41 loss: 0.19031772\n",
      "training batch 42 loss: 0.19031772\n",
      "training batch 43 loss: 0.19031772\n",
      "training batch 44 loss: 0.19031772\n",
      "training batch 45 loss: 0.19031772\n",
      "training batch 46 loss: 0.19031772\n",
      "training batch 47 loss: 0.19031772\n",
      "training batch 48 loss: 0.19031772\n",
      "training batch 49 loss: 0.19031772\n",
      "training batch 50 loss: 0.19031772\n",
      "training batch 51 loss: 0.19031772\n",
      "training batch 52 loss: 0.19031772\n",
      "training batch 53 loss: 0.19031772\n",
      "training batch 54 loss: 0.19031772\n",
      "training batch 55 loss: 0.19031772\n",
      "training batch 56 loss: 0.19031772\n",
      "training batch 57 loss: 0.19031772\n",
      "training batch 58 loss: 0.19031772\n",
      "training batch 59 loss: 0.19031772\n",
      "training batch 60 loss: 0.19031772\n",
      "training batch 61 loss: 0.19031772\n",
      "training batch 62 loss: 0.19031772\n",
      "training batch 63 loss: 0.19031772\n",
      "training batch 64 loss: 0.19031772\n",
      "training batch 65 loss: 0.19031772\n",
      "training batch 66 loss: 0.19031772\n",
      "training batch 67 loss: 0.19031772\n",
      "training batch 68 loss: 0.19031772\n",
      "training batch 69 loss: 0.19031772\n",
      "training batch 70 loss: 0.19031772\n",
      "training batch 71 loss: 0.19031772\n",
      "training batch 72 loss: 0.19031772\n",
      "training batch 73 loss: 0.19031772\n",
      "training batch 74 loss: 0.19031772\n",
      "training batch 75 loss: 0.19031772\n",
      "training batch 76 loss: 0.19031772\n",
      "training batch 77 loss: 0.19031772\n",
      "training batch 78 loss: 0.19031772\n",
      "training batch 79 loss: 0.19031772\n",
      "training batch 80 loss: 0.19031772\n",
      "training batch 81 loss: 0.19031772\n",
      "training batch 82 loss: 0.19031772\n",
      "training batch 83 loss: 0.19031772\n",
      "training batch 84 loss: 0.19031772\n",
      "training batch 85 loss: 0.19031772\n",
      "training batch 86 loss: 0.19031772\n",
      "training batch 87 loss: 0.19031772\n",
      "training batch 88 loss: 0.19031772\n",
      "training batch 89 loss: 0.19031772\n",
      "training batch 90 loss: 0.19031772\n",
      "training batch 91 loss: 0.19031772\n",
      "training batch 92 loss: 0.19031772\n",
      "training batch 93 loss: 0.19031772\n",
      "training batch 94 loss: 0.19031772\n",
      "training batch 95 loss: 0.19031772\n",
      "training batch 96 loss: 0.19031772\n",
      "training batch 97 loss: 0.19031776\n",
      "Test Loss: 0.19081613 \tSSIM score: 0.0118470285\n",
      "Learning rate: 0.25600000000000006\n",
      "training batch 0 loss: 0.19031772\n",
      "training batch 1 loss: 0.19031772\n",
      "training batch 2 loss: 0.19031772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 15:34:05.816598: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 3 loss: 0.19031772\n",
      "training batch 4 loss: 0.19031772\n",
      "training batch 5 loss: 0.19031772\n",
      "training batch 6 loss: 0.19031772\n",
      "training batch 7 loss: 0.19031772\n",
      "training batch 8 loss: 0.19031772\n",
      "training batch 9 loss: 0.19031772\n",
      "training batch 10 loss: 0.19031772\n",
      "training batch 11 loss: 0.19031772\n",
      "training batch 12 loss: 0.19031772\n",
      "training batch 13 loss: 0.19031772\n",
      "training batch 14 loss: 0.19031772\n",
      "training batch 15 loss: 0.19031772\n",
      "training batch 16 loss: 0.19031772\n",
      "training batch 17 loss: 0.19031772\n",
      "training batch 18 loss: 0.19031772\n",
      "training batch 19 loss: 0.19031772\n",
      "training batch 20 loss: 0.19031772\n",
      "training batch 21 loss: 0.19031772\n",
      "training batch 22 loss: 0.19031772\n",
      "training batch 23 loss: 0.19031772\n",
      "training batch 24 loss: 0.19031772\n",
      "training batch 25 loss: 0.19031772\n",
      "training batch 26 loss: 0.19031772\n",
      "training batch 27 loss: 0.19031772\n",
      "training batch 28 loss: 0.19031772\n",
      "training batch 29 loss: 0.19031772\n",
      "training batch 30 loss: 0.19031772\n",
      "training batch 31 loss: 0.19031772\n",
      "training batch 32 loss: 0.19031772\n",
      "training batch 33 loss: 0.19031772\n",
      "training batch 34 loss: 0.19031772\n",
      "training batch 35 loss: 0.19031772\n",
      "training batch 36 loss: 0.19031772\n",
      "training batch 37 loss: 0.19031772\n",
      "training batch 38 loss: 0.19031772\n",
      "training batch 39 loss: 0.19031772\n",
      "training batch 40 loss: 0.19031772\n",
      "training batch 41 loss: 0.19031772\n",
      "training batch 42 loss: 0.19031772\n",
      "training batch 43 loss: 0.19031772\n",
      "training batch 44 loss: 0.19031772\n",
      "training batch 45 loss: 0.19031772\n",
      "training batch 46 loss: 0.19031772\n",
      "training batch 47 loss: 0.19031772\n",
      "training batch 48 loss: 0.19031772\n",
      "training batch 49 loss: 0.19031772\n",
      "training batch 50 loss: 0.19031772\n",
      "training batch 51 loss: 0.19031772\n",
      "training batch 52 loss: 0.19031772\n",
      "training batch 53 loss: 0.19031772\n",
      "training batch 54 loss: 0.19031772\n",
      "training batch 55 loss: 0.19031772\n",
      "training batch 56 loss: 0.19031772\n",
      "training batch 57 loss: 0.19031772\n",
      "training batch 58 loss: 0.19031772\n",
      "training batch 59 loss: 0.19031772\n",
      "training batch 60 loss: 0.19031772\n",
      "training batch 61 loss: 0.19031772\n",
      "training batch 62 loss: 0.19031772\n",
      "training batch 63 loss: 0.19031772\n",
      "training batch 64 loss: 0.19031772\n",
      "training batch 65 loss: 0.19031772\n",
      "training batch 66 loss: 0.19031772\n",
      "training batch 67 loss: 0.19031772\n",
      "training batch 68 loss: 0.19031772\n",
      "training batch 69 loss: 0.19031772\n",
      "training batch 70 loss: 0.19031772\n",
      "training batch 71 loss: 0.19031772\n",
      "training batch 72 loss: 0.19031772\n",
      "training batch 73 loss: 0.19031772\n",
      "training batch 74 loss: 0.19031772\n",
      "training batch 75 loss: 0.19031772\n",
      "training batch 76 loss: 0.19031772\n",
      "training batch 77 loss: 0.19031772\n",
      "training batch 78 loss: 0.19031772\n",
      "training batch 79 loss: 0.19031772\n",
      "training batch 80 loss: 0.19031772\n",
      "training batch 81 loss: 0.19031772\n",
      "training batch 82 loss: 0.19031772\n",
      "training batch 83 loss: 0.19031772\n",
      "training batch 84 loss: 0.19031772\n",
      "training batch 85 loss: 0.19031772\n",
      "training batch 86 loss: 0.19031772\n",
      "training batch 87 loss: 0.19031772\n",
      "training batch 88 loss: 0.19031772\n",
      "training batch 89 loss: 0.19031772\n",
      "training batch 90 loss: 0.19031772\n",
      "training batch 91 loss: 0.19031772\n",
      "training batch 92 loss: 0.19031772\n",
      "training batch 93 loss: 0.19031772\n",
      "training batch 94 loss: 0.19031772\n",
      "training batch 95 loss: 0.19031772\n",
      "training batch 96 loss: 0.19031772\n",
      "training batch 97 loss: 0.19031776\n",
      "Test Loss: 0.19081613 \tSSIM score: 0.0118470285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean training loss</th>\n",
       "      <th>mean test loss</th>\n",
       "      <th>mean SSIM score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.21524148</td>\n",
       "      <td>0.0128818285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.19188926</td>\n",
       "      <td>0.19081613</td>\n",
       "      <td>0.0118470285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1903177</td>\n",
       "      <td>0.19081613</td>\n",
       "      <td>0.0118470285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1903177</td>\n",
       "      <td>0.19081613</td>\n",
       "      <td>0.0118470285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1903177</td>\n",
       "      <td>0.19081613</td>\n",
       "      <td>0.0118470285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mean training loss mean test loss mean SSIM score\n",
       "0               None     0.21524148    0.0128818285\n",
       "1         0.19188926     0.19081613    0.0118470285\n",
       "2          0.1903177     0.19081613    0.0118470285\n",
       "3          0.1903177     0.19081613    0.0118470285\n",
       "4          0.1903177     0.19081613    0.0118470285"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import train\n",
    "import pandas as pd\n",
    "\n",
    "learning_rate = 0.5\n",
    "num_epochs = 4\n",
    "batch_size = 512\n",
    "latent_vector_sizes = [3072]#[2*2048, 512, 64]\n",
    "param_init_scale = 0.1\n",
    "\n",
    "training_results_dict, params = train(num_epochs, latent_vector_sizes)\n",
    "training_results = pd.DataFrame(training_results_dict)\n",
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db67d978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 15:34:49.558174: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F51FC66F0D0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cifar10_loader import get_train_batches\n",
    "from train import make_sequence_batch, make_y_batch\n",
    "import numpy\n",
    "from PIL import Image\n",
    "\n",
    "for x in get_train_batches(batch_size=1):\n",
    "    y_batch = make_y_batch(x['image'])\n",
    "    seq_batch = make_sequence_batch(x['image'], sequence_length=1)\n",
    "    break\n",
    "img_array = numpy.concatenate([\n",
    "    numpy.asarray(numpy.reshape(seq_batch[:,0,:], (32, 32, 3)) * 256.0, dtype=numpy.uint8) \n",
    "    for i in range(seq_batch.shape[1])], axis=1)\n",
    "print(\"Input sequence:\")\n",
    "Image.fromarray(img_array).resize((128 * seq_batch.shape[1], 128), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764aecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F51FC70FFA0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import predict\n",
    "\n",
    "#image_reconstruction, error_prediction = predict(params, seq_batch[0,:])\n",
    "image_reconstruction = predict(params, seq_batch[0,:])\n",
    "#print(\"Error prediction:\", error_prediction)\n",
    "print(\"Reconstructed image:\")\n",
    "Image.fromarray(numpy.asarray(numpy.reshape(image_reconstruction, (32, 32, 3)) * 256.0, dtype=numpy.uint8)).resize((128, 128), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d570849c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F51FC6C5160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ground truth image:\")\n",
    "Image.fromarray(numpy.asarray(numpy.reshape(y_batch, (32, 32, 3)) * 256.0, dtype=numpy.uint8)).resize((128, 128), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e8388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
